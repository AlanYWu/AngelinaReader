{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "import local_config\n",
    "from ovotools import AttrDict\n",
    "params = AttrDict(data = AttrDict(net_hw = [758,758]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "mode = 'train'\n",
    "data_dir = os.path.join(local_config.data_path, 'DSBI') #My\n",
    "#BrailleDataset: def __init__(self, params, data_dir, mode):\n",
    "data_dir_data = os.path.join(data_dir, 'data') # ''\n",
    "list_file = os.path.join(data_dir, mode + '.txt')\n",
    "with open(list_file, 'r') as f:\n",
    "    files = f.readlines()\n",
    "files = [os.path.join(data_dir_data, fn.replace('.jpg\\n', '+recto')) for fn in files]\n",
    "#files = [os.path.join(data_dir_data, fn.replace('.jpg\\n', '')) for fn in files]\n",
    "assert len(files) > 0, list_file\n",
    "fn = files[15]\n",
    "#files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "import albumentations.augmentations.transforms as T\n",
    "import albumentations.augmentations.functional as F\n",
    "import cv2\n",
    "def common_aug(mode, params):\n",
    "    '''\n",
    "    :param mode: 'train', 'test'\n",
    "    '''\n",
    "    augs_list = []\n",
    "    augs_list += [albumentations.RandomCrop(params.data.net_hw[0], params.data.net_hw[1]),]\n",
    "    #augs_list += [albumentations.Normalize(mean=params.data.mean, std=params.data.std), ]\n",
    "    return albumentations.Compose(augs_list, p=1.) #, bbox_params = {'format':'albumentations', 'min_visibility':0.5}\n",
    "aug = common_aug('train', params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "img0 = np.asarray(PIL.Image.open(fn+'.jpg'))\n",
    "w = 768\n",
    "img = F.resize(img0, height=int(img0.shape[0]*w/img0.shape[1]), width=w)\n",
    "tst_w = img.shape[1]//2\n",
    "img_demo = img[100:100+tst_w//2, 150:100+tst_w]\n",
    "PIL.Image.fromarray(img_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_img = img\n",
    "# + rnd. Resize\n",
    "aug_img = T.ShiftScaleRotate(shift_limit=0, scale_limit=(-0.5,1.0), rotate_limit=(-5,5), border_mode=cv2.BORDER_CONSTANT, always_apply=False, p=1.)(image=aug_img)['image']\n",
    "aug_img = T.OpticalDistortion(border_mode=cv2.BORDER_CONSTANT, always_apply=False, p=1.)(image=aug_img)['image']\n",
    "#Crop\n",
    "aug_img = T.Blur(blur_limit=4, p=1.)(image=aug_img)['image']\n",
    "aug_img = T.RandomBrightnessContrast(always_apply=False, p=1.)(image=aug_img)['image']\n",
    "aug_img = T.MotionBlur(always_apply=False, p=1.)(image=aug_img)['image']\n",
    "aug_img = T.JpegCompression(quality_lower=30, quality_upper=100, always_apply=False, p=1.)(image=aug_img)['image']\n",
    "#H,V flip\n",
    "\n",
    "#img_demo = aug_img[100:100+tst_w//2, 150:100+tst_w]\n",
    "PIL.Image.fromarray(aug_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization (after augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "def Normalize(img):\n",
    "    '''\n",
    "    returns image converted to FloatTensor and normalized\n",
    "    '''\n",
    "    ten_img = F.to_tensor(img)\n",
    "    means = ten_img.view(3, -1).mean(dim=1)\n",
    "    std = ten_img.view(3, -1).std(dim=1)\n",
    "    ten_img = (ten_img-means.view(-1,1,1))/(3*std.view(-1,1,1))\n",
    "    # decolorize\n",
    "    ten_img = ten_img.mean(dim=0).expand(3,-1,-1)\n",
    "    return ten_img\n",
    "\n",
    "ten_img = Normalize(aug_img)\n",
    "\n",
    "import numpy as np\n",
    "def TensorToPilImage(tensor, mean, std):\n",
    "    vx_np = tensor.cpu().numpy().copy()\n",
    "    vx_np *= np.asarray(std)[:, np.newaxis, np.newaxis]\n",
    "    vx_np += np.asarray(mean)[:, np.newaxis, np.newaxis]\n",
    "    vx_np = vx_np.transpose(1,2,0).clip(0.0, 1.0)*255\n",
    "    return PIL.Image.fromarray(vx_np.astype(np.uint8))\n",
    "\n",
    "TensorToPilImage(ten_img, (0.5,), (0.2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import albumentations.augmentations.transforms as T\n",
    "import albumentations.augmentations.functional as albu_f\n",
    "import torchvision.transforms.functional as F\n",
    "import cv2\n",
    "import albumentations\n",
    "\n",
    "def common_aug(mode, params):\n",
    "    '''\n",
    "    :param mode: 'train', 'test'\n",
    "    '''\n",
    "    #aug_params = params.get('augm_params', dict())\n",
    "    augs_list = []\n",
    "    assert mode == 'train' # TODO #assert mode in {'train', 'test'}\n",
    "    if mode == 'train':\n",
    "        augs_list.append(T.Rotate(limit=5, border_mode=cv2.BORDER_CONSTANT, always_apply=True))\n",
    "        # augs_list.append(T.OpticalDistortion(border_mode=cv2.BORDER_CONSTANT)) - can't handle boundboxes\n",
    "        augs_list.append(albumentations.RandomCrop(416, 416, always_apply=True))\n",
    "        augs_list.append(T.Blur(blur_limit=4))\n",
    "        augs_list.append(T.RandomBrightnessContrast())\n",
    "        augs_list.append(T.MotionBlur())\n",
    "        augs_list.append(T.JpegCompression(quality_lower=30, quality_upper=100))\n",
    "        augs_list.append(T.VerticalFlip())\n",
    "        augs_list.append(T.HorizontalFlip())\n",
    "\n",
    "    return albumentations.Compose(augs_list, p=1., bbox_params = {'format':'albumentations', 'min_visibility':0.5})\n",
    "\n",
    "\n",
    "def random_resize_and_stretch(img, new_width_range, stretch_limit = 0):\n",
    "    new_width_range = T.to_tuple(new_width_range)\n",
    "    stretch_limit = T.to_tuple(stretch_limit, bias=1)\n",
    "    new_width = int(random.uniform(new_width_range[0], new_width_range[1]))\n",
    "    stretch = random.uniform(stretch_limit[0], stretch_limit[1])\n",
    "    new_height = int(img.shape[0]*new_width/img.shape[1]*stretch)\n",
    "    return albu_f.resize(img, height=new_height, width=new_width, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "aug = common_aug('train', None)\n",
    "aug_img = random_resize_and_stretch(img,\n",
    "                                    new_width_range=(614, 1840),\n",
    "                                    stretch_limit = 0.1)\n",
    "aug_img = aug(image = aug_img, bboxes = [])['image']\n",
    "\n",
    "ten_img = Normalize(aug_img)\n",
    "\n",
    "TensorToPilImage(ten_img, (0.5,), (0.2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
